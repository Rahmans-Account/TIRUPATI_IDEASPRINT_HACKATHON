# Model Configuration

random_forest:
  n_estimators: 500
  max_depth: 30
  min_samples_split: 10
  min_samples_leaf: 4
  max_features: "sqrt"
  n_jobs: -1
  random_state: 42
  class_weight: "balanced"
  
unet:
  architecture:
    in_channels: 9
    num_classes: 5
    encoder_depth: 4
    decoder_channels: [256, 128, 64, 32]
    activation: "relu"
    use_batchnorm: true
    dropout: 0.2
    
  training:
    batch_size: 16
    epochs: 100
    learning_rate: 0.001
    optimizer: "adam"
    weight_decay: 0.0001
    scheduler: "reduce_on_plateau"
    scheduler_patience: 10
    scheduler_factor: 0.5
    early_stopping_patience: 20
    
  loss:
    type: "combined"
    weights:
      cross_entropy: 0.5
      dice: 0.5
    class_weights: [1.0, 2.0, 1.0, 1.5, 1.5]
    
  augmentation:
    horizontal_flip: 0.5
    vertical_flip: 0.5
    rotation: 15
    brightness: 0.2
    contrast: 0.2
    
deeplabv3:
  backbone: "resnet50"
  output_stride: 16
  pretrained_backbone: true
  freeze_backbone: false
  
  training:
    batch_size: 8
    epochs: 80
    learning_rate: 0.0005
    optimizer: "adamw"
    weight_decay: 0.0001
    
ensemble:
  method: "weighted_average"
  models:
    - name: "random_forest"
      weight: 0.3
    - name: "unet"
      weight: 0.5
    - name: "deeplabv3"
      weight: 0.2
      
validation:
  split_ratio: 0.2
  stratified: true
  k_folds: 5
  random_state: 42
  
metrics:
  - "accuracy"
  - "precision"
  - "recall"
  - "f1_score"
  - "iou"
  - "confusion_matrix"
  
inference:
  batch_size: 32
  use_tta: false
  confidence_threshold: 0.6
  post_processing:
    morphological_closing: true
    kernel_size: 3
    remove_small_objects: true
    min_object_size: 50
